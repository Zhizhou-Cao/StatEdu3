---
title: 'Perception of Mathematical Question Difficulty: A Comparison Between Students
  and Experts'
author: "Zhizhou Cao, Rachel Wang, Yanxin Ruan, Lyuyin Yang"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(sirt)
library(ggpubr)
library(ggplot2)
library(correlation)
library(tibble)
library(knitr)
library(patchwork)
```

For code reproduction, please see: https://github.com/Zhizhou-Cao/StatEdu3 .

# Introduction

This project explores how well students and experts perceive the difficulty of mathematics questions, and how their subjective judgements align with actual student performance. This project utilises data from an undergraduate study in which 31 mathematical lectures (experts) and 83 first-year engineering students (students) were asked to compare pairs of mathematical diagnostic test questions. They were subsequently asked to determine which questions would likely be more difficult for incoming university students, where some groups saw questions with accompanying model solutions, whilst others did not.

In order to quantify the perceived difficulty, the Bradley-Terry model is applied to transforms pairwise comparisons into a continuous difficulty scale, which enabling us to investigate the following aspects:

1. To what extent does experts and students agree on the relative difficulty of questions?

2. How consistent does each group rank question difficulty?

3. How well does the perceived difficulty from both group align with actual difficulty?

To investigate these aspects, we conducted several additional analyses. The Pearson Correlation was used to assess the strength of association between expert and student judgements, and between perceived and actual difficulty. We further computed the sum of squared residuals (SSR) to investigate how well perceived difficulty predicts observed performance. Finally we used split-half reliability (SHR) to evaluate the internal consistency of each group’s judgements.

In addition to the overall analysis, we compared even-numbered and odd-numbered questions to test the robustness of our findings. Through this investigation, we aim to better understand how accurately students and experts can anticipate mathematical challenge, and explore the broader implications for diagnostic assessment design in higher education settings.



# Data 

This project utilises two datasets: 

-(1) Comparative judgement data from university students and mathematical lecturers.

-(2) Performance data from a diagnostic mathematics test taken by incoming first-year engineering students at the University of Edinburgh in 2020.

The judgement data consists of **nine** separate datasets. The expert group consists of mathematics lecturers and PhD students whilst the student groups consists of students in a first-year engineering mathematics course. In all cases, participants were presented with pairs of questions from the diagnostic test and was asked to indicate which of the two they believed would be more difficult for Year 1 students. These pairwise judgements were then used to infer an underlying difficulty ranking for the 20 test questions.

To explore the effects of question subset size on reliability, some groups were only shown either the odd-numbered or even-numbered questions, with pairings draw exclusively from within that subset. This was to investigate the hypothesis that restricting judges to fewer items might reduce cognitive load and improve the consistency of their comparisons. Furthermore, other groups such as `students_withoutsolutions`, were shown the test questions without model solutions, whereas all other groups had access to all model solutions during the task. Judges typically made 20 comparisons each, with the exception of the group contained in the dataset `students_withsolutions2`, where each judge made 40 comparisons. All datasets include between 10 and 83 judges, and comparisons span either 10 or 20 items depending on the condition.



# Expert vs Student (withsolution/even/odd)

To assess whether the judgements across different datasets were reliable and robust, we examined two key metrics. The Sum of Squared Residuals (SSR) was used to investigate how well the latent difficulty rankings predict actual performance, and the Split-Half Reliability (SHR) was utilised as an indication of the internal consistency of the comparative judgements within each group.

```{r import dataset, message=FALSE, echo=FALSE}
#'# ----Import dataset----
Actual_grade <- read_csv("ANON_2020-21diagtestSep.csv")

expert_even <- read_csv("experts-even.csv")
expert_odd <- read_csv("experts-odd.csv")
experts_withsolutions <- read_csv("experts-withsolutions.csv")

student_even <- read_csv("students-even.csv")
student_odd <- read_csv("students-odd.csv")

students_withoutsolutions <- read_csv("students-withoutsolutions.csv")

students_withsolutions <- read_csv("students-withsolutions.csv") # 20 judges, 20 comparisons
students_withsolutions1 <- read_csv("students-withsolutions1.csv") # 10 judges, 20 comparisons
students_withsolutions2 <- read_csv("students-withsolutions2.csv") # 10 judges, 40 comparisons

```

Following recommendations by Kinnear et al. (2025), a threshold of \(SSR \geq 0.8\) is considered a benchmark for acceptable reliability. In our analysis, the SSR values ranged from 0.813 (`students_withsolutions`) to 0.896 (`students_even`), which indicates strong reliability across all conditions. This implies that both students and experts were reasonably good at identifying which questions would be more or less difficult for incoming students. Notably, expert groups consistently produced slightly higher SSR scores compared to student with the exception of the Even questions, where students achieved the highest SSR score of 0.896. This may reflect the cognitive benefit of judging a smaller and more focused question set. Furthermore, we noticed high SSR values for the WithSolution dataset for experts and for students, which suggests that having access to worked solutions may have supported a more accurate difficulty estimation.

The SHR was further computed across the dataset, with values generally higher for experts compared to students, which indicates that experts tend to be more consistent in their judgements. The Even subset stood out once more as it showed the highest SHR values for both experts and students which further supports the idea that restricting item sets can enhance reliability when the overall number of comparisons remain sufficient. The lowest SHR was observed for the student Odd group of a value of 0.526, which falls below the commonly accepted 0.6 threshold, highlighting some variability in internal consistency, though it does not undermine the overall integrity of the scores. 





```{r ExpertvsStudent, echo=FALSE}
# WithSolution
# Expert model
sirt_messages <- capture.output(btm_results_expert_all <- sirt::btm(
  data = experts_withsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = experts_withsolutions$judge
))
btm_estimates_expert_all <- btm_results_expert_all$effects %>%
  select(individual, expert_theta = theta, expert_se = se.theta)

# Student model
sirt_messages <- capture.output(btm_results_student_all <- sirt::btm(
  data = students_withsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withsolutions$judge
))
btm_estimates_student_all <- btm_results_student_all$effects %>%
  select(individual, student_theta = theta, student_se = se.theta)

comparison_all <- left_join(btm_estimates_student_all, btm_estimates_expert_all, by = "individual")

# Even
# Expert

sirt_messages <- capture.output(btm_results_expert_even <- sirt::btm(
  data = expert_even %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = expert_even$judge
))
btm_estimates_expert_even <- btm_results_expert_even$effects %>%
  select(individual, expert_theta = theta)

# Student
sirt_messages <- capture.output(btm_results_student_even <- sirt::btm(
  data = student_even %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = student_even$judge
))
btm_estimates_student_even <- btm_results_student_even$effects %>%
  select(individual, student_theta = theta)

#odd
# Expert
sirt_messages <- capture.output(btm_results_expert_odd <- sirt::btm(
  data = expert_odd %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = expert_odd$judge
))
btm_estimates_expert_odd <- btm_results_expert_odd$effects %>%
  select(individual, expert_theta = theta)

# Student
sirt_messages <- capture.output(btm_results_student_odd <- sirt::btm(
  data = student_odd %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = student_odd$judge
))
btm_estimates_student_odd <- btm_results_student_odd$effects %>%
  select(individual, student_theta = theta)

```


# Table of SSR and SHR Values
```{r SSR/SHR table, echo=FALSE}
# SSR
ssr_table <- tibble(
  Dataset = c("WithSolution", "Even", "Odd"),
  Expert_SSR = c(
    btm_results_expert_all$sepG^2 / (1 + btm_results_expert_all$sepG^2),
    btm_results_expert_even$sepG^2 / (1 + btm_results_expert_even$sepG^2),
    btm_results_expert_odd$sepG^2 / (1 + btm_results_expert_odd$sepG^2)
  ),
  Student_SSR = c(
    btm_results_student_all$sepG^2 / (1 + btm_results_student_all$sepG^2),
    btm_results_student_even$sepG^2 / (1 + btm_results_student_even$sepG^2),
    btm_results_student_odd$sepG^2 / (1 + btm_results_student_odd$sepG^2)
  )
)
# SHR
compute_split_half_irr <- function(decisions_data) {
  decisions <- decisions_data %>%
    mutate(winning_column = 1) %>%
    select(candidate_chosen, candidate_not_chosen, winning_column, judge)
  
  judge_group1 <- decisions %>% distinct(judge) %>% slice_sample(prop = 0.5)
  judge_group2 <- decisions %>% distinct(judge) %>% anti_join(judge_group1, by = "judge")
  
  judgements1 <- decisions %>% semi_join(judge_group1, by = "judge")
  judgements2 <- decisions %>% semi_join(judge_group2, by = "judge")
  
  btm1 <- purrr::quietly(sirt::btm)(judgements1 %>% data.frame, maxit = 400, fix.eta = 0)$result
  btm2 <- purrr::quietly(sirt::btm)(judgements2 %>% data.frame, maxit = 400, fix.eta = 0)$result
  
  merged <- merge(btm1$effects, btm2$effects, by = "individual")
  return(cor(merged$theta.x, merged$theta.y, method = "pearson"))
}

# deine dataset
sh_data_list <- list(
  WithSolution_Expert = experts_withsolutions,
  WithSolution_Student = students_withsolutions,
  Even_Expert = expert_even,
  Even_Student = student_even,
  Odd_Expert = expert_odd,
  Odd_Student = student_odd
)

# SHR
set.seed(10108)
shr_results <- map_df(names(sh_data_list), function(name) {
  data <- sh_data_list[[name]]
  shr_vals <- replicate(100, compute_split_half_irr(data))
  tibble(Group = name, SHR = median(shr_vals))
})

shr_results_split <- shr_results %>%
  separate(Group, into = c("Dataset", "Group"), sep = "_")

# SSR
ssr_long <- ssr_table %>%
  pivot_longer(cols = c(Expert_SSR, Student_SSR),
               names_to = "Group",
               names_pattern = "(.*)_SSR",
               values_to = "SSR")

# Combine table
reliability_table <- left_join(ssr_long, shr_results_split, by = c("Dataset", "Group")) %>%
  select(Dataset, Group, SSR, SHR)

kable(reliability_table)
```

# Table of Correlation Values
We computed the pearson correlation values between three data sets and found little differences, they are all significant as shown below. 
```{r correlation, echo=FALSE}
comparison_all <- left_join(btm_estimates_student_all, btm_estimates_expert_all, by = "individual")
comparison_even <- left_join(btm_estimates_student_even, btm_estimates_expert_even, by = "individual")
comparison_odd <- left_join(btm_estimates_student_odd, btm_estimates_expert_odd, by = "individual")

# Correlation 
cor_withsolution <- correlation(comparison_all %>% select(expert_theta, student_theta), method = "pearson") %>%
  mutate(Group = "WithSolution")
cor_even <- correlation(comparison_even %>% select(expert_theta, student_theta), method = "pearson") %>%
  mutate(Group = "Even")
cor_odd <- correlation(comparison_odd %>% select(expert_theta, student_theta), method = "pearson") %>%
  mutate(Group = "Odd")

# Combine
combined_result <- bind_rows(cor_withsolution, cor_even, cor_odd)

# tidy 
combined_result <- combined_result %>% 
  select(Group, Parameter1, Parameter2, r, CI_low, CI_high, t, df_error, p)

# print
kable(combined_result)

```

We also visualised the relationship between experts and students, with error bar displayed. There exists a positive relationship.
```{r expert vs student plot, echo=FALSE, message=FALSE}
# Merge & Plot


comparison_all %>%
  ggplot(aes(x = expert_theta, y = student_theta)) +
  geom_point() +
  geom_text(aes(label = individual), hjust = -0.1, vjust = 0, size = 3) +
  geom_errorbar(aes(ymin = student_theta - student_se, ymax = student_theta + student_se), width = 0.05) +
  geom_errorbarh(aes(xmin = expert_theta - expert_se, xmax = expert_theta + expert_se), height = 0.05) +
  geom_smooth(method = "lm", se = FALSE, color = "gray") +
  ggpubr::stat_cor() +
  labs(title = "With Solution: Expert vs Student",
       x = "Expert Theta", y = "Student Theta")+
  theme_minimal()


```

We also compared the Bradley-Terry model with the raw scores for expert vs student with full solutions, and again noticed little differences. 
```{r Raw visual, echo=FALSE, eval=FALSE}
# expert_data add judge_group = "expert"
experts_withsolutions<- experts_withsolutions %>%
  mutate(judge_group = "expert")

# student_data add judge_group = "student"
students_withsolutions <- students_withsolutions %>%
  mutate(judge_group = "student")

withsolution_combined <- bind_rows(experts_withsolutions, students_withsolutions)
rigour_btm_results <- withsolution_combined %>% 
  nest(.by = judge_group) %>% 
  mutate(
    btm_results = purrr::map(data, \(x)
                             purrr::quietly(sirt::btm)(
                               x %>% transmute(candidate_chosen, candidate_not_chosen, col = 1) %>% data.frame,
                               maxit = 400,
                               fix.eta = 0,
                               ignore.ties = TRUE
                             )$result
    )
  ) %>% 
  select(-data)

rigour_scores <- rigour_btm_results %>% 
  mutate(effects = purrr::map(btm_results, \(x) x$effects)) %>% 
  unnest(effects) %>% 
  select(-btm_results)

rigour_scores %>% 
  filter(judge_group %in% c("expert", "student")) %>% 
  select(judge_group, id, propscore, theta) %>% 
  pivot_longer(cols = c(propscore, theta), names_to = "method") %>% 
  mutate(method = case_match(method, "theta" ~ "Bradley-Terry", "propscore" ~ "Raw score")) %>% 
  pivot_wider(names_from = judge_group, values_from = value) %>% 
  ggplot(aes(x = expert, y = student)) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "#999999", linewidth = 0.2) +
  geom_point(size = 2) +
  geom_text(aes(label = id), vjust = 1, hjust = 0, nudge_x = 0.01, nudge_y = -0.01, color = "black") +
  ggpubr::stat_cor() +
  facet_wrap(~ method, scales = "free")+
  theme_minimal()

```
Overall, these reliability metrics support the robustness of our findings and confirms that the comparative judgement tasks generated consistent scores across different judgement groups and item sets, providing reassurance that our use of CJ provides a valid and reliable method for inferring perceived difficulty in mathematical diagnostic assessments.


# Actual vs (Expert and Student)

To assess actual question difficulty, we used performance data from the 2020–21 Mathematics Diagnostic Test administered at the University of Edinburgh. The test, delivered online via an e-assessment platform, covered core high school mathematics topics including algebra, trigonometry, and calculus. Students completed the assessment in a single 90-minute sitting. Although participation was optional, it was encouraged for all incoming undergraduates, and only complete and valid attempts were included in the analysis.

Mean scores for each of the 20 questions were calculated to reflect actual difficulty. Lower average scores indicate that fewer students answered the question correctly, and thus reflect greater difficulty. These scores represent observed student performance and serve as a baseline for comparison. We then compared these rankings to those derived from Bradley-Terry models of expert and student judgements to assess how well their perceived difficulty aligned with actual difficulty.

```{r Actual data, message=FALSE, echo=FALSE}
# Read the data
Actual_grade <- read_csv("ANON_2020-21diagtestSep.csv", na = c("", "NA", "-"))

# Select only the question columns
grade_questions <- Actual_grade %>% select(6:25)

# Force conversion to numeric (exclude "-" entries)
grade_questions <- grade_questions %>%
  mutate(across(everything(), ~ as.numeric(.)))

# Calculate total score, number of responses, mean, and standard deviation for each question
average_scores <- grade_questions %>%
  summarise(across(
    everything(),
    list(
      total = ~ sum(., na.rm = TRUE),
      count = ~ sum(!is.na(.)),
      average = ~ mean(., na.rm = TRUE),
      sd = ~ sd(., na.rm = TRUE)
    ),
    .names = "{.col}_{.fn}"
  )) %>%
  pivot_longer(cols = everything(),
               names_to = c("Question", ".value"),
               names_sep = "_")

# Add standard error of the mean (SEM) column
average_scores <- average_scores %>%
  mutate(Question = str_extract(Question, "\\d+")) %>%
  mutate(sem = sd / sqrt(count))

# View results
# print(average_scores)
```

```{r Actual Visualisation, message=FALSE, echo = FALSE}
# Merge average scores with expert theta + standard error
expert_vs_actual <- btm_estimates_expert_all %>%
  mutate(Question = paste0(individual)) %>%
  left_join(average_scores %>% mutate(Question = str_extract(Question, "\\d+")), by = "Question")

# Merge average scores with student theta + standard error
student_vs_actual <- btm_estimates_student_all %>%
  mutate(Question = paste0(individual)) %>%
  left_join(average_scores %>% mutate(Question = str_extract(Question, "\\d+")), by = "Question")

# Expert plot object
plot_expert <- expert_vs_actual %>%
  ggplot(aes(x = average, y = expert_theta)) +
  geom_point(size = 2) +
  geom_text(aes(label = Question), hjust = -0.1, vjust = 0, size = 3) +
  geom_errorbar(aes(ymin = expert_theta - expert_se, ymax = expert_theta + expert_se), width = 0.05) +
  geom_errorbarh(aes(xmin = average - sem, xmax = average + sem), height = 0.05) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  ggpubr::stat_cor(method = "pearson") +
  labs(title = "Expert-Perceived Difficulty vs Actual Grade",
    x = "Average Score (Actual Performance)",
    y = "Expert-Estimated Difficulty (θ)") + 
  theme_minimal()

# Student plot object
plot_student <- student_vs_actual %>%
  ggplot(aes(x = average, y = student_theta)) +
  geom_point(size = 2) +
  geom_text(aes(label = Question), hjust = -0.1, vjust = 0, size = 3) +
  geom_errorbar(aes(ymin = student_theta - student_se, ymax = student_theta + student_se), width = 0.05) +
  geom_errorbarh(aes(xmin = average - sem, xmax = average + sem), height = 0.05) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  ggpubr::stat_cor(method = "pearson") +
  labs(title = "Student-Perceived Difficulty vs Actual Grade",
    x = "Average Score (Actual Performance)",
    y = "Student-Estimated Difficulty (θ)") +
  theme_minimal()

# Display the two plots stacked vertically
plot(plot_expert / plot_student)
```

To evaluate alignment between perceived and actual difficulty, we correlated each group’s estimated difficulty scores (\(\theta\)) with the average performance scores across questions. The visualisation results reveal a clear negative relationship in both cases: questions perceived as more difficult (higher \(\theta\)) tend to have lower average performance scores, indicating greater actual difficulty.

For the expert group, this relationship was strong (\(r = –0.71, p < 0.001\)), indicating that expert judgments closely mirrored student performance. The student group also exhibited a moderate but statistically significant negative correlation (\(r = –0.54, p = 0.014\)), suggesting that students were reasonably able to identify which questions were more challenging, albeit with greater variability.

These findings suggest that both experts and students have a good understanding of question difficulty, with experts showing a closer alignment with actual student performance, due to experience. While overall patterns were consistent, certain items deviated from expectations. For example, question 10 was perceived as difficult but performed well, whereas question 14 was underestimated by students. Therefore, the difference between individual questions should be considered as a factor for further analysis. 


# Comparison Within Student Datasets:

While the primary aim of this project is to investigate how well students and experts can judge the difficulty of mathematical questions, it is also important to explore what factors influence the quality and reliability of student judgements. To that end, we conducted two supplementary analyses comparing different groups of students under other conditions: 

To evaluate whether access to model solutions affected students’ perception of item difficulty, we compared two groups of students—those with and without access to solutions. In addition, the comparison between the “with_solution” and “with_solution2” groups, who both completed an equal number of comparisons (400), allowed us to assess the reliability and stability of students’ judgements under controlled conditions.


## Students: Comparing With vs Without Solution

To compare the perceived difficulty rankings between students who had access to model solutions and those who did not, we examined their Bradley-Terry (BT) \(\theta\) estimates across all 20 items. 

We first examined their Pearson correlation table shown below. The resulting estimates showed a strong and significant correlation between the two groups, with r = 0.83, 95% CI = [0.60, 0.93], p<0.001. This suggests that students’ overall rankings of question difficulty were largely consistent regardless of whether model solutions were available.

```{r With-without data, echo=FALSE}


# Fit BT model for WITH solution
sirt_messages <- capture.output(btm_students_with <- sirt::btm(
  data = students_withsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withsolutions$judge
))
theta_with <- btm_students_with$effects %>%
  select(individual, theta_with = theta, se_with = se.theta)

# Fit BT model for WITHOUT solution
sirt_messages <- capture.output(btm_students_without <- sirt::btm(
  data = students_withoutsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withoutsolutions$judge
))
theta_without <- btm_students_without$effects %>%
  select(individual, theta_without = theta, se_without = se.theta)

```

```{r with-without correlation, echo=FALSE}
# Merge results
theta_compare <- left_join(theta_with, theta_without, by = "individual")
# Correlation test
correlation::correlation(
  theta_compare %>% select(theta_with, theta_without),
  method = "pearson"
)


```

SSR value reflects how well the estimated difficulties align with actual student performance. Students' SSR were 0.813 (with solution) and 0.816 (without solution). Similarly, SHR values, a measure of internal consistency, were 0.583 and 0.580, respectively. These near-identical values suggest that solution access had little impact on either reliability or predictive accuracy of the rankings.

```{r with-without SHR-SSR, echo=FALSE}
students_with <- students_withsolutions %>%
  mutate(judge_group = "with_solution")
students_without <- students_withoutsolutions %>%
  mutate(judge_group = "without_solution")
# Combine two student groups
student_combined <- bind_rows(students_with, students_without)

# === SHR function (same as before) ===
compute_split_half_irr <- function(decisions_data) {
  decisions <- decisions_data %>%
    mutate(winning_column = 1) %>%
    select(candidate_chosen, candidate_not_chosen, winning_column, judge)
  
  judge_group1 <- decisions %>% distinct(judge) %>% slice_sample(prop = 0.5)
  judge_group2 <- decisions %>% distinct(judge) %>% anti_join(judge_group1, by = "judge")
  
  judgements1 <- decisions %>% semi_join(judge_group1, by = "judge")
  judgements2 <- decisions %>% semi_join(judge_group2, by = "judge")
  
  btm1 <- purrr::quietly(sirt::btm)(judgements1 %>% data.frame, maxit = 400, fix.eta = 0)$result
  btm2 <- purrr::quietly(sirt::btm)(judgements2 %>% data.frame, maxit = 400, fix.eta = 0)$result
  
  merged <- merge(btm1$effects, btm2$effects, by = "individual")
  return(cor(merged$theta.x, merged$theta.y, method = "pearson"))
}

# === Compute SHR for both student groups ===
set.seed(10108)
shr_student_solution <- student_combined %>%
  nest(.by = judge_group) %>%
  mutate(
    split_half_irr = purrr::map(data, \(x) replicate(100, compute_split_half_irr(x)))
  ) %>%
  unnest(split_half_irr) %>%
  summarise(SHR = median(split_half_irr), .by = judge_group)


# =========================
# Compute SSR
# =========================

# Fit BTM model separately to get sepG values for SSR
# === SSR for students_with ===
sirt_messages_with <- capture.output(
  btm_students_with <- sirt::btm(
    data = students_with %>%
      select(candidate_chosen, candidate_not_chosen) %>%
      mutate(result = 1) %>%
      data.frame(),
    maxiter = 400,
    fix.eta = 0,
    judge = students_with$judge
  )
)

# === SSR for students_without ===
sirt_messages_without <- capture.output(
  btm_students_without <- sirt::btm(
    data = students_without %>%
      select(candidate_chosen, candidate_not_chosen) %>%
      mutate(result = 1) %>%
      data.frame(),
    maxiter = 400,
    fix.eta = 0,
    judge = students_without$judge
  )
)

# Compute SSR values
ssr_student_solution <- tibble(
  judge_group = c("with_solution", "without_solution"),
  SSR = c(
    btm_students_with$sepG^2 / (1 + btm_students_with$sepG^2),
    btm_students_without$sepG^2 / (1 + btm_students_without$sepG^2)
  )
)

# =========================
# -----Combine SSR + SHR------
# =========================
reliability_student_solution <- left_join(
  ssr_student_solution,
  shr_student_solution,
  by = "judge_group"
) %>%
  mutate(Dataset = case_when(
    judge_group == "with_solution" ~ "With Solution",
    judge_group == "without_solution" ~ "Without Solution"
  )) %>%
  select(Dataset, SSR, SHR)

# Print final reliability table
kable(reliability_student_solution)
```

The graph below displays the perceived difficulty rankings for each question under both conditions. Due to reordering the questions by \(\theta\) values, the with-solution group exhibits a smooth pattern. In contrast, the without-solution group shows a more irregular pattern and a less monotonic structure, suggesting greater variability or uncertainty in their difficulty judgements. Despite the pattern variance, the two groups have similar upward trends. 

```{r with-without plot, echo=FALSE}
btm_results_students <- student_combined %>%
  nest(.by = judge_group) %>%
  mutate(
    btm_results = purrr::map(data, \(x)
                             purrr::quietly(sirt::btm)(
                               x %>% transmute(candidate_chosen, candidate_not_chosen, col = 1) %>% data.frame,
                               maxit = 400,
                               fix.eta = 0,
                               ignore.ties = TRUE
                             )$result
    )
  ) %>%
  select(-data)
# === Extract theta scores ===
theta_scores_students <- btm_results_students %>%
  mutate(effects = purrr::map(btm_results, \(x) x$effects)) %>%
  unnest(effects) %>%
  select(-btm_results)

theta_scores_students %>%
  left_join(
    theta_scores_students %>%
      filter(judge_group == "with_solution") %>%
      select(id, with_theta = theta),
    join_by(id)
  ) %>%
  mutate(
    proof = as.factor(id) %>% fct_reorder(with_theta),
    judge_group = fct_relevel(judge_group, "with_solution", "without_solution")
  ) %>%
  ggplot(aes(x = theta, y = proof)) +
  geom_point() +
  facet_grid(cols = vars(judge_group)) +
  labs(title = "Perceived Difficulty by Students (With vs Without Solution)",
       x = "Theta", y = "Proof")+
  theme_minimal()
```

To summarise, we compared the difficulty rankings between students who completed the task with and without model solutions. The Pearson correlation between the two groups' theta estimates was high, indicating strong agreement in overall rankings. The reliability table was also similar. These results suggest that access to solutions had minimal impact on both the accuracy and consistency of student judgements. 


## Students: Compare with Solution vs Solution2 (more judges)

To further explore how the distribution of judgment workload might affect reliability, we focused on these two groups. Both judged all 20 questions with model solutions. Dataset `students_withsolutions` had more judges (20) making fewer comparisons, while dataset `students_withsolutions2` had fewer judges (10) making more comparisons. Importantly, the total number of comparisons was kept approximately the same at 400.

```{r withsol data, echo=FALSE}
# ===============================
# Fit BTM model for students_withsolutions
# ===============================
sirt_messages <- capture.output(btm_results_studentswithsolution <- sirt::btm(
  data = students_withsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withsolutions$judge
))
btm_estimates_studentswithsolution <- btm_results_studentswithsolution$effects %>%
  select(individual, theta_withsol = theta, se_withsol = se.theta)

# ===============================
# Fit BTM model for students_withsolutions2
# ===============================
sirt_messages <- capture.output(btm_results_studentswithsolution2 <- sirt::btm(
  data = students_withsolutions2 %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withsolutions2$judge
))
btm_estimates_studentswithsolution2 <- btm_results_studentswithsolution2$effects %>%
  select(individual, theta_withsol2 = theta, se_withsol2 = se.theta)

# ===============================
# Merge & Compare
# ===============================
comparison_withsolution <- left_join(btm_estimates_studentswithsolution, btm_estimates_studentswithsolution2, by = "individual")
```

The Pearson correlation between the two theta estimates was moderate, r = 0.64, p = 0.002, suggesting a reasonable level of agreement. Though it was notably lower than the correlation observed between the with and without solution groups. This indicates some variability in difficulty rankings between the two groups.

```{r withsol correlation, echo=FALSE}
# Correlation
correlation::correlation(
  comparison_withsolution %>% select(theta_withsol, theta_withsol2),
  method = "pearson"
)
```


The two groups show extremely strong positive correlations (both 0.996) between raw values and BT models coefficients, indicating minimal differences. 
```{r correlaion table, echo=FALSE}
# ===============================
# SECTION 1: SSR - students_withsolutions & students_withsolutions2
# ===============================
ssr_compare_table <- tibble(
  Dataset = c("students_withsolutions", "students_withsolutions2"),
  SSR = c(
    btm_results_studentswithsolution$sepG^2 / (1 + btm_results_studentswithsolution$sepG^2),
    btm_results_studentswithsolution2$sepG^2 / (1 + btm_results_studentswithsolution2$sepG^2)
  )
)



# ===============================
# SECTION 2: SHR - students_withsolutions & students_withsolutions2
# ===============================
compute_split_half_irr <- function(decisions_data) {
  decisions <- decisions_data %>%
    mutate(winning_column = 1) %>%
    select(candidate_chosen, candidate_not_chosen, winning_column, judge)
  
  judge_group1 <- decisions %>% distinct(judge) %>% slice_sample(prop = 0.5)
  judge_group2 <- decisions %>% distinct(judge) %>% anti_join(judge_group1, by = "judge")
  
  judgements1 <- decisions %>% semi_join(judge_group1, by = "judge")
  judgements2 <- decisions %>% semi_join(judge_group2, by = "judge")
  
  btm1 <- purrr::quietly(sirt::btm)(judgements1 %>% data.frame(), maxit = 400, fix.eta = 0)$result
  btm2 <- purrr::quietly(sirt::btm)(judgements2 %>% data.frame(), maxit = 400, fix.eta = 0)$result
  
  merged <- merge(btm1$effects, btm2$effects, by = "individual")
  return(cor(merged$theta.x, merged$theta.y, method = "pearson"))
}

set.seed(10108)
shr_compare_table <- tibble(
  Dataset = c("students_withsolutions", "students_withsolutions2"),
  SHR = c(
    median(replicate(100, compute_split_half_irr(students_withsolutions))),
    median(replicate(100, compute_split_half_irr(students_withsolutions2)))
  )
)

# ===============================
# SECTION 3: Raw Score vs Theta
# ===============================
compare_raw_vs_theta <- function(btm_result) {
  btm_result$effects %>%
    select(theta, propscore) %>%
    correlation::correlation(method = "pearson", p_adjust = "none")
}

raw_vs_theta_list <- list(
  Studentswithsolution = btm_results_studentswithsolution,
  Studentswithsolution2 = btm_results_studentswithsolution2
)

raw_vs_theta_table <- map_df(names(raw_vs_theta_list), function(name) {
  res <- correlation::correlation(
    raw_vs_theta_list[[name]]$effects %>% select(theta, propscore),
    method = "pearson", p_adjust = "none"
  )
  tibble(
    Dataset = name,
    r = round(res$r, 3),
    CI = paste0("[", round(res$CI_low, 3), ", ", round(res$CI_high, 3), "]"),
    p = signif(res$p, 3)
  )
})
kable(raw_vs_theta_table)
```

Reliability table of the two groups differed. While SSR remained relatively high in both groups (0.813 and 0.757), SHR for group "withsolutions" (0.583) is significantly higher than "withsolutions2" (0.259). This suggests that increasing the number of comparisons per student may have introduced more inconsistency, possibly due to fatigue, reduced attention, or diminishing engagement over time.
```{r ssr+shr, echo=FALSE}
# ===============================
# SECTION 4: Combine SSR + SHR
# ===============================
reliability_table <- left_join(ssr_compare_table, shr_compare_table, by = "Dataset")
kable(reliability_table)
```

When comparing 20 and 40 comparisons per student by visualisation, the ranking of perceived difficulty was more stable in the 20-comparison condition. The theta estimates showed a clear and consistent ordering, while in the 40-comparison condition, the ranking became more scattered and inconsistent. This aligns with the much lower SHR value (0.26), suggesting that more comparisons did not improve, and may have reduced, the reliability of student judgements.

```{r withsol plot, echo=FALSE}
# Add group label to each set of theta estimates
theta_withsol_long <- btm_estimates_studentswithsolution %>%
  mutate(judge_group = "20 comparisons") %>%
  rename(theta = theta_withsol, se = se_withsol)

theta_withsol2_long <- btm_estimates_studentswithsolution2 %>%
  mutate(judge_group = "40 comparisons") %>%
  rename(theta = theta_withsol2, se = se_withsol2)

# Combine into one long dataset
theta_combined <- bind_rows(theta_withsol_long, theta_withsol2_long)

# Use 20-comparison group to define theta-based ordering of individuals
theta_order <- theta_withsol_long %>%
  arrange(theta) %>%
  pull(individual)

# Plot
theta_combined %>%
  mutate(
    individual = factor(individual, levels = theta_order),
    judge_group = factor(judge_group, levels = c("20 comparisons", "40 comparisons"))
  ) %>%
  ggplot(aes(x = theta, y = individual)) +
  geom_point() +
  facet_grid(cols = vars(judge_group)) +
  labs(
    title = "Perceived Difficulty by Students (20 vs 40 Comparisons)",
    x = "Theta Estimate",
    y = "Candidate"
  ) +
  theme_minimal()

```

Thus, students with access to model solutions produced moderately consistent difficulty rankings across two datasets, as reflected by a correlation of R=0.64 and SSR > 0.75. However, the notable drop in SHR from 0.58 to 0.26 suggests reduced reliability in individual judgements when students made more comparisons. This indicates that increasing the number of comparisons may introduce inconsistency, even when solutions are provided.

To conclude, students’ difficulty judgments were broadly consistent with or without access to model solutions, with similar SSR and SHR values. However, when the same total number of comparisons was completed by fewer students, internal consistency (SHR) dropped substantially. This suggests that while model solutions have limited impact, distributing comparisons across more students leads to more reliable judgments.


# Limitations

Although both expert and student theta scores showed statistically significant correlations with actual average grades, these were only analysed at the level of question means. This meant that we were unable to conduct item-level residual analysis due to the limited number of data points with n = 20 test questions, nor fit a regression model. From Kinnear et al. (2025), more robust analyses often require a larger number of items or groups of judges to support finer-grained statistical modelling. 

Secondly, despite the fact that our use of the Bradley-Terry model provided a principled way to convert pairwise comparisons into continuous difficult estimates, it assumes unidimensionality and does not directly account for the non-linear effects or interactions such as specific content areas or the way the questions are formatted. Furthermore, due to the low sample size and risk of multicollinearity which would make model convergence unreliable, we were unable to fit a regression model to predict perceived difficulty from features such as average score, topic, and visual complexity. This constraints our ability to fully explore content validity, which is a key aspect of CJ evaluation (Jones & Davies, 2024).

Finally, while we report correlations and reliability estimates, we were unable to compute the true inter-rater reliability (IRR) due to the absence of an independent replication group. Instead, we used split-half reliability (SHR) as a proxy, which is known to systematically underestimate IRR (Verhavert et al., 2018). Several student groups showed SHR values below the commonly used 0.7 threshold, which suggests that there is some variability and reduced internal consistency in their difficulty rankings. This indicates that while student judgements are broadly informative, they may be less stable across repeated samples compared to expert judgements.



# Conclusion

This project aimed to investigate how well experts and students can perceive the difficulty of mathematics questions and how these perceptions relate to actual student performance. By applying the Bradley-Terry model to comparative judgment data from both groups, we were able to quantify difficulty on a continuous scale and address the three key research questions mentioned.

Firstly, we examined to the extent of which the experts and students agreed in their rankings of question difficulty. We observed moderate but statistically significant agreement between student and expert difficulty rankings with a correlation of r = 0.63 with model solutions, which suggests partial shared understanding. However, differences in judgement likely reflect variations in experience, pedagogical insight, and confidence.

Secondly, we assessed the internal consistency of each group’s judgement by using SHR. We noticed that expert groups produced more internally consistent rankings which was shown by their higher SHR values across most datasets.  Student SHR values varied more widely, with some groups falling below accepted reliability thresholds. This reinforces findings from the literature that expert judgements are typically more stable and coherent.

Thirdly, we evaluated how well the perceived difficulty aligned with actual student performance by examining the Pearson correlation. We noticed that expert-perceived difficulty showed a stronger inverse correlation of r = -0.71 with average question scores compared to student judgements with an inverse correlation of r = -0.54, which indicates that experts were able to better anticipate which questions student found the most challenging. This was further supported by the SSR values, which were generally higher for expert groups across all conditions, which suggests that their rankings was the most predictive of observed difficulty. 

The analysis of the odd and even numbered question subsets confirmed the robustness of the results by showing that the main findings were consistent across different item groupings.  While the limited number of test items (n = 20) precluded more complex regression modelling, the convergence of results across correlation analyses, SHR and SSR provides strong evidence that experts make more consistent and accurate judgements of question difficulty than students. 

Overall, this project demonstrates that comparative judgement paired with the Bradley-Terry model is a powerful and reliable method for quantifying perceived difficulty and understanding how students and experts interpret mathematical challenge.  These findings highlight the value of expert input in diagnostic assessment design and the potential to use comparative judgement as a tool for investigating metacognitive awareness among students.


# References

Jones, I., & Davies, B. (2024). Comparative judgement in education research. International Journal of Research & Method in Education, 47(2), 170181. https://doi.org/10.1080/1743727X.2023.2242273

Kinnear, G., Jones, I., & Davies, B. (2025). Comparative judgement as a research tool: A meta-analysis of application and reliability. https://doi.org/10.31219/osf.io/c9q3b_v1

Verhavert, S., De Maeyer, S., Donche, V., & Coertjens, L. (2018). Scale separation reliability: What does it mean in the context of comparative judgment? Applied Psychological Measurement, 42(6), 428–445. https://doi.org/10.1177/0146621617748321

