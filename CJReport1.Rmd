---
title: "CJReport1"
author: "Zhizhou Cao"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(sirt)
library(ggpubr)
library(ggplot2)
library(correlation)
library(tibble)
library(knitr)
library(patchwork)
```


```{r import dataset, message=FALSE}
#'# ----Import dataset----
Actual_grade <- read_csv("ANON_2020-21diagtestSep.csv")

expert_even <- read_csv("experts-even.csv")
expert_odd <- read_csv("experts-odd.csv")
experts_withsolutions <- read_csv("experts-withsolutions.csv")

student_even <- read_csv("students-even.csv")
student_odd <- read_csv("students-odd.csv")

students_withoutsolutions <- read_csv("students-withoutsolutions.csv")

students_withsolutions <- read_csv("students-withsolutions.csv") # 20 judges, 20 comparisons
students_withsolutions1 <- read_csv("students-withsolutions1.csv") # 10 judges, 20 comparisons
students_withsolutions2 <- read_csv("students-withsolutions2.csv") # 10 judges, 40 comparisons

```

# Expert vs Student
```{r ExpertvsStudent}
# WithSolution
# Expert model
sirt_messages <- capture.output(btm_results_expert_all <- sirt::btm(
  data = experts_withsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = experts_withsolutions$judge
))
btm_estimates_expert_all <- btm_results_expert_all$effects %>%
  select(individual, expert_theta = theta, expert_se = se.theta)

# Student model
sirt_messages <- capture.output(btm_results_student_all <- sirt::btm(
  data = students_withsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withsolutions$judge
))
btm_estimates_student_all <- btm_results_student_all$effects %>%
  select(individual, student_theta = theta, student_se = se.theta)

comparison_all <- left_join(btm_estimates_student_all, btm_estimates_expert_all, by = "individual")

# Even
# Expert

sirt_messages <- capture.output(btm_results_expert_even <- sirt::btm(
  data = expert_even %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = expert_even$judge
))
btm_estimates_expert_even <- btm_results_expert_even$effects %>%
  select(individual, expert_theta = theta)

# Student
sirt_messages <- capture.output(btm_results_student_even <- sirt::btm(
  data = student_even %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = student_even$judge
))
btm_estimates_student_even <- btm_results_student_even$effects %>%
  select(individual, student_theta = theta)

#odd
# Expert
sirt_messages <- capture.output(btm_results_expert_odd <- sirt::btm(
  data = expert_odd %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = expert_odd$judge
))
btm_estimates_expert_odd <- btm_results_expert_odd$effects %>%
  select(individual, expert_theta = theta)

# Student
sirt_messages <- capture.output(btm_results_student_odd <- sirt::btm(
  data = student_odd %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = student_odd$judge
))
btm_estimates_student_odd <- btm_results_student_odd$effects %>%
  select(individual, student_theta = theta)

```



```{r SSR/SHR table}
# SSR
ssr_table <- tibble(
  Dataset = c("WithSolution", "Even", "Odd"),
  Expert_SSR = c(
    btm_results_expert_all$sepG^2 / (1 + btm_results_expert_all$sepG^2),
    btm_results_expert_even$sepG^2 / (1 + btm_results_expert_even$sepG^2),
    btm_results_expert_odd$sepG^2 / (1 + btm_results_expert_odd$sepG^2)
  ),
  Student_SSR = c(
    btm_results_student_all$sepG^2 / (1 + btm_results_student_all$sepG^2),
    btm_results_student_even$sepG^2 / (1 + btm_results_student_even$sepG^2),
    btm_results_student_odd$sepG^2 / (1 + btm_results_student_odd$sepG^2)
  )
)
# SHR
compute_split_half_irr <- function(decisions_data) {
  decisions <- decisions_data %>%
    mutate(winning_column = 1) %>%
    select(candidate_chosen, candidate_not_chosen, winning_column, judge)
  
  judge_group1 <- decisions %>% distinct(judge) %>% slice_sample(prop = 0.5)
  judge_group2 <- decisions %>% distinct(judge) %>% anti_join(judge_group1, by = "judge")
  
  judgements1 <- decisions %>% semi_join(judge_group1, by = "judge")
  judgements2 <- decisions %>% semi_join(judge_group2, by = "judge")
  
  btm1 <- purrr::quietly(sirt::btm)(judgements1 %>% data.frame, maxit = 400, fix.eta = 0)$result
  btm2 <- purrr::quietly(sirt::btm)(judgements2 %>% data.frame, maxit = 400, fix.eta = 0)$result
  
  merged <- merge(btm1$effects, btm2$effects, by = "individual")
  return(cor(merged$theta.x, merged$theta.y, method = "pearson"))
}

# 定义每组数据集
sh_data_list <- list(
  WithSolution_Expert = experts_withsolutions,
  WithSolution_Student = students_withsolutions,
  Even_Expert = expert_even,
  Even_Student = student_even,
  Odd_Expert = expert_odd,
  Odd_Student = student_odd
)

# 多次 split-half 重复估计
set.seed(10108)
shr_results <- map_df(names(sh_data_list), function(name) {
  data <- sh_data_list[[name]]
  shr_vals <- replicate(100, compute_split_half_irr(data))
  tibble(Group = name, SHR = median(shr_vals))
})

shr_results_split <- shr_results %>%
  separate(Group, into = c("Dataset", "Group"), sep = "_")

# 同样，把 SSR 表整理为长格式
ssr_long <- ssr_table %>%
  pivot_longer(cols = c(Expert_SSR, Student_SSR),
               names_to = "Group",
               names_pattern = "(.*)_SSR",
               values_to = "SSR")

# 合并两个表
reliability_table <- left_join(ssr_long, shr_results_split, by = c("Dataset", "Group")) %>%
  select(Dataset, Group, SSR, SHR)

kable(reliability_table)
```

```{r correlation}
comparison_all <- left_join(btm_estimates_student_all, btm_estimates_expert_all, by = "individual")
comparison_even <- left_join(btm_estimates_student_even, btm_estimates_expert_even, by = "individual")
comparison_odd <- left_join(btm_estimates_student_odd, btm_estimates_expert_odd, by = "individual")

cat("\n=== Correlation Summary ===\n")
list(
  WithSolution = correlation::correlation(comparison_all %>% select(expert_theta, student_theta), method = "pearson"),
  Even = correlation::correlation(comparison_even %>% select(expert_theta, student_theta), method = "pearson"),
  Odd = correlation::correlation(comparison_odd %>% select(expert_theta, student_theta), method = "pearson")
)

```

```{r expert vs student plot}
# Merge & Plot


comparison_all %>%
  ggplot(aes(x = expert_theta, y = student_theta)) +
  geom_point() +
  geom_text(aes(label = individual), hjust = -0.1, vjust = 0, size = 3) +
  geom_errorbar(aes(ymin = student_theta - student_se, ymax = student_theta + student_se), width = 0.05) +
  geom_errorbarh(aes(xmin = expert_theta - expert_se, xmax = expert_theta + expert_se), height = 0.05) +
  geom_smooth(method = "lm", se = FALSE, color = "gray") +
  ggpubr::stat_cor() +
  labs(title = "With Solution: Expert vs Student",
       x = "Expert Theta", y = "Student Theta")


```


```{r Raw visual}
# 给 expert_data 添加 judge_group = "expert"
experts_withsolutions<- experts_withsolutions %>%
  mutate(judge_group = "expert")

# 给 student_data 添加 judge_group = "student"
students_withsolutions <- students_withsolutions %>%
  mutate(judge_group = "student")

withsolution_combined <- bind_rows(experts_withsolutions, students_withsolutions)
rigour_btm_results <- withsolution_combined %>% 
  nest(.by = judge_group) %>% 
  mutate(
    btm_results = purrr::map(data, \(x)
                             purrr::quietly(sirt::btm)(
                               x %>% transmute(candidate_chosen, candidate_not_chosen, col = 1) %>% data.frame,
                               maxit = 400,
                               fix.eta = 0,
                               ignore.ties = TRUE
                             )$result
    )
  ) %>% 
  select(-data)

rigour_scores <- rigour_btm_results %>% 
  mutate(effects = purrr::map(btm_results, \(x) x$effects)) %>% 
  unnest(effects) %>% 
  select(-btm_results)

rigour_scores %>% 
  filter(judge_group %in% c("expert", "student")) %>% 
  select(judge_group, id, propscore, theta) %>% 
  pivot_longer(cols = c(propscore, theta), names_to = "method") %>% 
  mutate(method = case_match(method, "theta" ~ "Bradley-Terry", "propscore" ~ "Raw score")) %>% 
  pivot_wider(names_from = judge_group, values_from = value) %>% 
  ggplot(aes(x = expert, y = student)) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "#999999", linewidth = 0.2) +
  geom_point(size = 2) +
  geom_text(aes(label = id), vjust = 1, hjust = 0, nudge_x = 0.01, nudge_y = -0.01, color = "black") +
  ggpubr::stat_cor() +
  facet_wrap(~ method, scales = "free")

```


# Actual vs (Experts/Student)

To assess actual question difficulty, we used performance data from the 2020–21 Mathematics Diagnostic Test administered at the University of Edinburgh. The test, delivered online via an e-assessment platform, covered core high school mathematics topics including algebra, trigonometry, and calculus. Students completed the assessment in a single 90-minute sitting. Although participation was optional, it was encouraged for all incoming undergraduates, and only complete and valid attempts were included in the analysis.

Mean scores for each of the 20 questions were calculated to reflect actual difficulty. Lower average scores indicate that fewer students answered the question correctly, and thus reflect greater difficulty. These scores represent observed student performance and serve as a baseline for comparison. We then compared these rankings to those derived from Bradley-Terry models of expert and student judgements to assess how well their perceived difficulty aligned with actual difficulty.

```{r Actual data, message=FALSE, echo=FALSE}
# Read the data
Actual_grade <- read_csv("ANON_2020-21diagtestSep.csv", na = c("", "NA", "-"))

# Select only the question columns
grade_questions <- Actual_grade %>% select(6:25)

# Force conversion to numeric (exclude "-" entries)
grade_questions <- grade_questions %>%
  mutate(across(everything(), ~ as.numeric(.)))

# Calculate total score, number of responses, mean, and standard deviation for each question
average_scores <- grade_questions %>%
  summarise(across(
    everything(),
    list(
      total = ~ sum(., na.rm = TRUE),
      count = ~ sum(!is.na(.)),
      average = ~ mean(., na.rm = TRUE),
      sd = ~ sd(., na.rm = TRUE)
    ),
    .names = "{.col}_{.fn}"
  )) %>%
  pivot_longer(cols = everything(),
               names_to = c("Question", ".value"),
               names_sep = "_")

# Add standard error of the mean (SEM) column
average_scores <- average_scores %>%
  mutate(Question = str_extract(Question, "\\d+")) %>%
  mutate(sem = sd / sqrt(count))

# View results
# print(average_scores)
```

```{r Actual Visualisation, message=FALSE, echo = FALSE}
# Merge average scores with expert theta + standard error
expert_vs_actual <- btm_estimates_expert_all %>%
  mutate(Question = paste0(individual)) %>%
  left_join(average_scores %>% mutate(Question = str_extract(Question, "\\d+")), by = "Question")

# Merge average scores with student theta + standard error
student_vs_actual <- btm_estimates_student_all %>%
  mutate(Question = paste0(individual)) %>%
  left_join(average_scores %>% mutate(Question = str_extract(Question, "\\d+")), by = "Question")

# Expert plot object
plot_expert <- expert_vs_actual %>%
  ggplot(aes(x = average, y = expert_theta)) +
  geom_point(size = 2) +
  geom_text(aes(label = Question), hjust = -0.1, vjust = 0, size = 3) +
  geom_errorbar(aes(ymin = expert_theta - expert_se, ymax = expert_theta + expert_se), width = 0.05) +
  geom_errorbarh(aes(xmin = average - sem, xmax = average + sem), height = 0.05) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  ggpubr::stat_cor(method = "pearson") +
  labs(title = "Expert-Perceived Difficulty vs Actual Grade",
    x = "Average Score (Actual Performance)",
    y = "Expert-Estimated Difficulty (θ)") + 
  theme_minimal()

# Student plot object
plot_student <- student_vs_actual %>%
  ggplot(aes(x = average, y = student_theta)) +
  geom_point(size = 2) +
  geom_text(aes(label = Question), hjust = -0.1, vjust = 0, size = 3) +
  geom_errorbar(aes(ymin = student_theta - student_se, ymax = student_theta + student_se), width = 0.05) +
  geom_errorbarh(aes(xmin = average - sem, xmax = average + sem), height = 0.05) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  ggpubr::stat_cor(method = "pearson") +
  labs(title = "Student-Perceived Difficulty vs Actual Grade",
    x = "Average Score (Actual Performance)",
    y = "Student-Estimated Difficulty (θ)") +
  theme_minimal()

# Display the two plots stacked vertically
plot(plot_expert / plot_student)
```

To evaluate alignment between perceived and actual difficulty, we correlated each group’s estimated difficulty scores (θ) with the average performance scores across questions. The resulting visualizations reveal a clear negative relationship in both cases: questions perceived as more difficult (higher θ) tend to have lower average performance scores, indicating greater actual difficulty.

For the expert group, this relationship was strong (r = –0.71, p < 0.001), indicating that expert judgments closely mirrored student performance. The student group also exhibited a moderate but statistically significant negative correlation (r = –0.54, p = 0.014), suggesting that students were reasonably able to identify which questions were more challenging, albeit with greater variability.

Together, these findings suggest that both experts and students possess a meaningful understanding of relative item difficulty, with experts showing a closer alignment with actual student performance. This supports the value of expert judgments in test design and question calibration, while also demonstrating that students—despite their limited experience—can make reasonably accurate comparative evaluations of question difficulty. The stronger alignment observed among experts may reflect their greater subject knowledge and pedagogical experience. While overall patterns were consistent, certain items deviated from expectations—for example, question 10 was perceived as difficult but performed well, whereas question 14 was underestimated by students. These exceptions highlight the importance of combining statistical models with expert insight when interpreting item performance. Besides, the analysis is based on a relatively small set of 20 questions, so results should be interpreted with caution and viewed as indicative rather than definitive.


# With vs Without Solution

```{r With-without data}


# Fit BT model for WITH solution
sirt_messages <- capture.output(btm_students_with <- sirt::btm(
  data = students_withsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withsolutions$judge
))
theta_with <- btm_students_with$effects %>%
  select(individual, theta_with = theta, se_with = se.theta)

# Fit BT model for WITHOUT solution
sirt_messages <- capture.output(btm_students_without <- sirt::btm(
  data = students_withoutsolutions %>%
    select(candidate_chosen, candidate_not_chosen) %>%
    mutate(result = 1) %>%
    data.frame(),
  maxiter = 400,
  fix.eta = 0,
  judge = students_withoutsolutions$judge
))
theta_without <- btm_students_without$effects %>%
  select(individual, theta_without = theta, se_without = se.theta)

```

```{r with-without correlation}
# Merge results
theta_compare <- left_join(theta_with, theta_without, by = "individual")
# Correlation test
cor.test(theta_compare$theta_with, theta_compare$theta_without)


```

```{r with-without SHR-SSR}
students_with <- students_withsolutions %>%
  mutate(judge_group = "with_solution")
students_without <- students_withoutsolutions %>%
  mutate(judge_group = "without_solution")
# Combine two student groups
student_combined <- bind_rows(students_with, students_without)

# === SHR function (same as before) ===
compute_split_half_irr <- function(decisions_data) {
  decisions <- decisions_data %>%
    mutate(winning_column = 1) %>%
    select(candidate_chosen, candidate_not_chosen, winning_column, judge)
  
  judge_group1 <- decisions %>% distinct(judge) %>% slice_sample(prop = 0.5)
  judge_group2 <- decisions %>% distinct(judge) %>% anti_join(judge_group1, by = "judge")
  
  judgements1 <- decisions %>% semi_join(judge_group1, by = "judge")
  judgements2 <- decisions %>% semi_join(judge_group2, by = "judge")
  
  btm1 <- purrr::quietly(sirt::btm)(judgements1 %>% data.frame, maxit = 400, fix.eta = 0)$result
  btm2 <- purrr::quietly(sirt::btm)(judgements2 %>% data.frame, maxit = 400, fix.eta = 0)$result
  
  merged <- merge(btm1$effects, btm2$effects, by = "individual")
  return(cor(merged$theta.x, merged$theta.y, method = "pearson"))
}

# === Compute SHR for both student groups ===
set.seed(10108)
shr_student_solution <- student_combined %>%
  nest(.by = judge_group) %>%
  mutate(
    split_half_irr = purrr::map(data, \(x) replicate(100, compute_split_half_irr(x)))
  ) %>%
  unnest(split_half_irr) %>%
  summarise(SHR = median(split_half_irr), .by = judge_group)


# =========================
# Compute SSR
# =========================

# Fit BTM model separately to get sepG values for SSR
# === SSR for students_with ===
sirt_messages_with <- capture.output(
  btm_students_with <- sirt::btm(
    data = students_with %>%
      select(candidate_chosen, candidate_not_chosen) %>%
      mutate(result = 1) %>%
      data.frame(),
    maxiter = 400,
    fix.eta = 0,
    judge = students_with$judge
  )
)

# === SSR for students_without ===
sirt_messages_without <- capture.output(
  btm_students_without <- sirt::btm(
    data = students_without %>%
      select(candidate_chosen, candidate_not_chosen) %>%
      mutate(result = 1) %>%
      data.frame(),
    maxiter = 400,
    fix.eta = 0,
    judge = students_without$judge
  )
)

# Compute SSR values
ssr_student_solution <- tibble(
  judge_group = c("with_solution", "without_solution"),
  SSR = c(
    btm_students_with$sepG^2 / (1 + btm_students_with$sepG^2),
    btm_students_without$sepG^2 / (1 + btm_students_without$sepG^2)
  )
)

# =========================
# -----Combine SSR + SHR------
# =========================
reliability_student_solution <- left_join(
  ssr_student_solution,
  shr_student_solution,
  by = "judge_group"
) %>%
  mutate(Dataset = case_when(
    judge_group == "with_solution" ~ "With Solution",
    judge_group == "without_solution" ~ "Without Solution"
  )) %>%
  select(Dataset, SSR, SHR)

# Print final reliability table
kable(reliability_student_solution)
```

```{r with-without plot}
btm_results_students <- student_combined %>%
  nest(.by = judge_group) %>%
  mutate(
    btm_results = purrr::map(data, \(x)
                             purrr::quietly(sirt::btm)(
                               x %>% transmute(candidate_chosen, candidate_not_chosen, col = 1) %>% data.frame,
                               maxit = 400,
                               fix.eta = 0,
                               ignore.ties = TRUE
                             )$result
    )
  ) %>%
  select(-data)
# === Extract theta scores ===
theta_scores_students <- btm_results_students %>%
  mutate(effects = purrr::map(btm_results, \(x) x$effects)) %>%
  unnest(effects) %>%
  select(-btm_results)

theta_scores_students %>%
  left_join(
    theta_scores_students %>%
      filter(judge_group == "with_solution") %>%
      select(id, with_theta = theta),
    join_by(id)
  ) %>%
  mutate(
    proof = as.factor(id) %>% fct_reorder(with_theta),
    judge_group = fct_relevel(judge_group, "with_solution", "without_solution")
  ) %>%
  ggplot(aes(x = theta, y = proof)) +
  geom_point() +
  facet_grid(cols = vars(judge_group)) +
  labs(title = "Perceived Difficulty by Students (With vs Without Solution)",
       x = "Theta", y = "Proof")
```
